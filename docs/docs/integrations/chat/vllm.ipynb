{
 "cells": [
  {
   "cell_type": "raw",
   "id": "eb65deaa",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "sidebar_label: vLLM Chat\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7e5679-aa06-47e4-a1a3-b6b70e604017",
   "metadata": {},
   "source": [
    "# vLLM Chat\n",
    "\n",
    "vLLM can be deployed as a server that mimics the OpenAI API protocol. This allows vLLM to be used as a drop-in replacement for applications using OpenAI API. This server can be queried in the same format as OpenAI API.\n",
    "\n",
    "This notebook covers how to get started with vLLM chat models using langchain's `ChatOpenAI` **as it is**, as well as how to do structured generation with `ChatVLLMOpenAI`.\n",
    "\n",
    "We assume you already have a vLLM server running. See the [vLLM README](https://docs.vllm.ai/en/latest/serving/deploying_with_docker.html) for instructions on how to deploy a vLLM server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "060a2e3d-d42f-4221-bd09-a9a06544dcd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatVLLMOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf24d732-68a9-44fd-b05d-4903ce5620c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The URL of the vLLM inference server\n",
    "inference_server_url = \"http://localhost:8000/v1\"\n",
    "\n",
    "llm = ChatVLLMOpenAI(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    openai_api_base=inference_server_url,\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea4e363-5688-4b07-82ed-6aa8153c2377",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mi piace programmazione.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant that translates English to Italian. Respond with only the translation.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Translate the following sentence from English to Italian: I love programming.\"\n",
    "    ),\n",
    "]\n",
    "llm.invoke(messages).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fc7046-a6dc-4720-8c0c-24a6db76a4f4",
   "metadata": {},
   "source": [
    "You can make use of templating by using a `ChatPromptTemplate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "123980e9-0dee-4ce5-bde6-d964dd90129c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mi piace il programmazione.\n",
      "J'adore le programmation.\n"
     ]
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}. Respond with only the translation.\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "    ]\n",
    ")\n",
    "chain = chat_prompt | llm | StrOutputParser()\n",
    "print(\n",
    "    chain.invoke(\n",
    "        {\n",
    "            \"input_language\": \"English\",\n",
    "            \"output_language\": \"Italian\",\n",
    "            \"input\": \"I love programming.\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    chain.invoke(\n",
    "        {\n",
    "            \"input_language\": \"English\",\n",
    "            \"output_language\": \"French\",\n",
    "            \"input\": \"I love programming.\",\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2841b5ef",
   "metadata": {},
   "source": [
    "# Structured Output With Guided Generation\n",
    "\n",
    "The `ChatVLLMOpenAI` class supports the `with_structured_output` method for structuring the output of the model.\n",
    "This is achieved through vLLM' support for guided generation.\n",
    "This is a very powerful feature that allows even small models to accurately follow instructions.\n",
    "Below we show several examples of how to use this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb63a774",
   "metadata": {},
   "source": [
    "We can use ensure that the model output can be parsed as a pydantic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4414161a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Paris' population=2141000 country='France' population_category='>1M'\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# Example pydantic model\n",
    "class CityModel(BaseModel):\n",
    "    name: str = Field(..., description=\"Name of the city\")\n",
    "    population: int = Field(\n",
    "        ..., description=\"Population of the city measured in number of inhabitants\"\n",
    "    )\n",
    "    country: str = Field(..., description=\"Country of the city\")\n",
    "    population_category: Literal[\">1M\", \"<1M\"] = Field(\n",
    "        ..., description=\"Population category of the city\"\n",
    "    )\n",
    "\n",
    "\n",
    "structured_llm = llm.with_structured_output(CityModel, method=\"function_calling\")\n",
    "\n",
    "city_model = structured_llm.invoke(\"What is the capital of France?\")\n",
    "assert isinstance(city_model, CityModel)\n",
    "print(city_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48740e14",
   "metadata": {},
   "source": [
    "vLLM has more extensive structuring methods.\n",
    "For example, we can ensure that the model output is one of a set of options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e221e3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "allowed_choices = [\"positive\", \"negative\"]\n",
    "structured_llm = llm.with_structured_output(\n",
    "    allowed_choices,\n",
    "    method=\"guided_choice\",\n",
    ")\n",
    "\n",
    "print(structured_llm.invoke(\"I loved this movie!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc401ee6",
   "metadata": {},
   "source": [
    "We can use regex too. For example to ensure that the output is a valid hex color code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c17dd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FF0000\n"
     ]
    }
   ],
   "source": [
    "regex = r\"#[0-9a-fA-F]{6}\"\n",
    "structured_llm = llm.with_structured_output(\n",
    "    regex,\n",
    "    method=\"guided_regex\",\n",
    ")\n",
    "\n",
    "print(structured_llm.invoke(\"Give me the code for red.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21bc68b",
   "metadata": {},
   "source": [
    "vLLM even supports EBNF grammars. Such as this example that restricts output to simple mathematical equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3f6a350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256-324/3\n"
     ]
    }
   ],
   "source": [
    "grammar = \"\"\"?start: expression\n",
    "\n",
    "?expression: term ((\"+\" | \"-\") term)*\n",
    "\n",
    "?term: factor ((\"*\" | \"/\") factor)*\n",
    "\n",
    "?factor: NUMBER\n",
    "        | \"-\" factor\n",
    "        | \"(\" expression \")\"\n",
    "\n",
    "%import common.NUMBER\"\"\"\n",
    "\n",
    "structured_llm = llm.with_structured_output(\n",
    "    grammar,\n",
    "    method=\"guided_grammar\",\n",
    ")\n",
    "print(\n",
    "    structured_llm.invoke(\n",
    "        \"Translate two hundred and fifty-six minus three hundred and twenty-four divided by three into a mathematical expression.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa64ece",
   "metadata": {},
   "source": [
    "If you just want to ensure the output is valid JSON, you can select \"json_mode\" and specify `None` for the `schema`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "040f19c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': 'Paris'}\n"
     ]
    }
   ],
   "source": [
    "structured_llm = llm.with_structured_output(None, method=\"json_mode\")\n",
    "print(structured_llm.invoke(\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6894e555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'properties': {'capital': {'type': 'string'}}, 'required': ['capital']}\n"
     ]
    }
   ],
   "source": [
    "structured_llm = llm.with_structured_output(None, method=\"guided_json\")\n",
    "print(structured_llm.invoke(\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0752a20e",
   "metadata": {},
   "source": [
    "There is also support for `bind_tools`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d826100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '',\n",
       " 'additional_kwargs': {'tool_calls': [{'id': 'chatcmpl-tool-71a325323dda4660ba88f9fd2105ee36',\n",
       "    'function': {'arguments': '{ \"name\": \"Paris\", \"population\": 2140000, \"country\": \"France\", \"population_category\": \">1M\" }',\n",
       "     'name': 'CityModel'},\n",
       "    'type': 'function'}]},\n",
       " 'response_metadata': {'token_usage': {'completion_tokens': 33,\n",
       "   'prompt_tokens': 17,\n",
       "   'total_tokens': 50},\n",
       "  'model_name': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
       "  'system_fingerprint': None,\n",
       "  'finish_reason': 'stop',\n",
       "  'logprobs': None},\n",
       " 'type': 'ai',\n",
       " 'name': None,\n",
       " 'id': 'run-328ed011-383d-48fe-92a2-b1da67e5ed45-0',\n",
       " 'example': False,\n",
       " 'tool_calls': [{'name': 'CityModel',\n",
       "   'args': {'name': 'Paris',\n",
       "    'population': 2140000,\n",
       "    'country': 'France',\n",
       "    'population_category': '>1M'},\n",
       "   'id': 'chatcmpl-tool-71a325323dda4660ba88f9fd2105ee36'}],\n",
       " 'invalid_tool_calls': [],\n",
       " 'usage_metadata': {'input_tokens': 17,\n",
       "  'output_tokens': 33,\n",
       "  'total_tokens': 50}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "tool_llm = llm.bind_tools([CityModel], tool_choice=\"CityModel\")\n",
    "res = tool_llm.invoke(\"What is the capital of France?\")\n",
    "json.loads(res.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4b4f72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
