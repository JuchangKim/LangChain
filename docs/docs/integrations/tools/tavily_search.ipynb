{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f91f20",
   "metadata": {},
   "source": [
    "# Tavily Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e24a889",
   "metadata": {},
   "source": [
    "[Tavily's Search API](https://tavily.com) is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.\n",
    "\n",
    "## Setup\n",
    "\n",
    "The integration lives in the `langchain-community` package. We also need to install the `tavily-python` package itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f85b4089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --quiet -U langchain-community tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e9266",
   "metadata": {},
   "source": [
    "### Credentials\n",
    "\n",
    "We also need to set our Tavily API key. You can get an API key by visiting [this site](https://app.tavily.com/sign-in) and creating an account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b178a2-8816-40ca-b57c-ccdd86dde9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5ab717-fd27-4c59-b912-bdd099541478",
   "metadata": {},
   "source": [
    "It's also helpful (but not needed) to set up [LangSmith](https://smith.langchain.com/) for best-in-class observability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6c2f136-6367-4f1f-825d-ae741e1bf281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c97218f-f366-479d-8bf7-fe9f2f6df73f",
   "metadata": {},
   "source": [
    "## Instantiation\n",
    "\n",
    "Here we show how to instatiate an instance of the Tavily search tools, with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b3ddfe9-ca79-494c-a7ab-1f56d9407a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tool = TavilySearchResults(\n",
    "    max_results=3,\n",
    "    search_depth=\"basic\",\n",
    "    include_domains = [\"https://langchain.com\"],\n",
    "    exclude_domains = [\"https://braintrust.com\"],\n",
    "    include_answer = False,\n",
    "    include_raw_content = False,\n",
    "    include_images = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74147a1a",
   "metadata": {},
   "source": [
    "## Invocation\n",
    "\n",
    "We can invoke the tool in two ways. One is with a normal query, and the other is by providing it with a `ToolCall`. If we invoke it with a normal query, only the content of the tool will be returned. If we invoke it with a `ToolCall`, a `ToolMessage` will be returned.\n",
    "\n",
    "### Invoke with normal query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65310a8b-eb0c-4d9e-a618-4f4abe2414fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What happened in the latest burning man floods?', 'output': 'In the latest Burning Man festival, heavy rains caused flooding and resulted in thousands of attendees being stranded. The festival took place in Black Rock Forest, Nevada, and around 70,000 people were gathered for the event., {'input': 'What happened in the latest burning man floods?', 'output': 'The latest Burning Man festival experienced heavy rain, resulting in floods and muddy conditions. Thousands of attendees were stranded at the festival site in Nevada. There were false claims of an Ebola outbreak and a national emergency, but no emergency declaration was made., langchain_community.adapters.openai.convert_message_to_dict(message: BaseMessage) → dict [source] ¶. Convert a LangChain message to a dictionary. Parameters. message ( BaseMessage) - The LangChain message. Returns.\n"
     ]
    }
   ],
   "source": [
    "ans = tool.invoke({\"query\": \"What happened in the latest burning man floods\"})\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e73897",
   "metadata": {},
   "source": [
    "### Invoke with ToolCall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f90e33a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1722043273, \\'localtime\\': \\'2024-07-26 18:21\\'}, \\'current\\': {\\'last_updated_epoch\\': 1722042900, \\'last_updated\\': \\'2024-07-26 18:15\\', \\'temp_c\\': 15.2, \\'temp_f\\': 59.4, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Sunny\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/113.png\\', \\'code\\': 1000}, \\'wind_mph\\': 14.3, \\'wind_kph\\': 23.0, \\'wind_degree\\': 258, \\'wind_dir\\': \\'WSW\\', \\'pressure_mb\\': 1012.0, \\'pressure_in\\': 29.88, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 77, \\'cloud\\': 2, \\'feelslike_c\\': 14.5, \\'feelslike_f\\': 58.1, \\'windchill_c\\': 14.5, \\'windchill_f\\': 58.1, \\'heatindex_c\\': 15.2, \\'heatindex_f\\': 59.4, \\'dewpoint_c\\': 11.4, \\'dewpoint_f\\': 52.6, \\'vis_km\\': 10.0, \\'vis_miles\\': 6.0, \\'uv\\': 5.0, \\'gust_mph\\': 17.7, \\'gust_kph\\': 28.5}}, Log a trace. No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. Python. TypeScript. from langchain_openai import ChatOpenAI. from langchain_core.prompts import ChatPromptTemplate. from langchain_core.output_parsers import StrOutputParser. prompt = ChatPromptTemplate.from_messages([., To save your prompt, click the \"Save as\" button, name your prompt, and decide if you want it to be \"private\" or \"public\". Private prompts are only visible to your workspace, while public prompts are discoverable to anyone in the LangChain Hub. Click save to create your prompt. The model and configuration you select in the Playground settings ...' name='tavily_search_results_json' tool_call_id='foo' artifact={'query': 'weather in sf?', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Weather in San Francisco', 'url': 'https://www.weatherapi.com/', 'content': \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.78, 'lon': -122.42, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1722043273, 'localtime': '2024-07-26 18:21'}, 'current': {'last_updated_epoch': 1722042900, 'last_updated': '2024-07-26 18:15', 'temp_c': 15.2, 'temp_f': 59.4, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 14.3, 'wind_kph': 23.0, 'wind_degree': 258, 'wind_dir': 'WSW', 'pressure_mb': 1012.0, 'pressure_in': 29.88, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 77, 'cloud': 2, 'feelslike_c': 14.5, 'feelslike_f': 58.1, 'windchill_c': 14.5, 'windchill_f': 58.1, 'heatindex_c': 15.2, 'heatindex_f': 59.4, 'dewpoint_c': 11.4, 'dewpoint_f': 52.6, 'vis_km': 10.0, 'vis_miles': 6.0, 'uv': 5.0, 'gust_mph': 17.7, 'gust_kph': 28.5}}\", 'score': 0.393096, 'raw_content': None}, {'title': 'Trace with LangChain (Python and JS/TS) | ️ ️ LangSmith', 'url': 'https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain', 'content': 'Log a trace. No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. Python. TypeScript. from langchain_openai import ChatOpenAI. from langchain_core.prompts import ChatPromptTemplate. from langchain_core.output_parsers import StrOutputParser. prompt = ChatPromptTemplate.from_messages([.', 'score': 0.04451443, 'raw_content': None}, {'title': 'Create a prompt | ️ ️ LangSmith', 'url': 'https://docs.smith.langchain.com/how_to_guides/prompts/create_a_prompt', 'content': 'To save your prompt, click the \"Save as\" button, name your prompt, and decide if you want it to be \"private\" or \"public\". Private prompts are only visible to your workspace, while public prompts are discoverable to anyone in the LangChain Hub. Click save to create your prompt. The model and configuration you select in the Playground settings ...', 'score': 0.025711587, 'raw_content': None}], 'response_time': 1.74}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import ToolCall\n",
    "\n",
    "# A ToolCall is just a typeddict, so this is the same as saying tool.invoke({\"args\": {'query': 'weather in sf?'}, \"type\": \"tool_call\", \"id\": \"foo\", \"name\": \"tavily\"})\n",
    "ans = tool.invoke(ToolCall(args={\"query\":\"weather in sf?\"},id=\"foo\", name=\"tavily\",type=\"tool_call\"))\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee4096f",
   "metadata": {},
   "source": [
    "## Tool Return Objects\n",
    "\n",
    "As part of our recent overhaul of tools, we are moving towards using a content/artifact paradigm for `ToolMessages`. The content will be a string and the artifact will contain the raw information of the tool call. \n",
    "\n",
    "In the case of the Tavily tool, our string is just a concatenation of the contents from each search result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "394feda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1722043273, \\'localtime\\': \\'2024-07-26 18:21\\'}, \\'current\\': {\\'last_updated_epoch\\': 1722042900, \\'last_updated\\': \\'2024-07-26 18:15\\', \\'temp_c\\': 15.2, \\'temp_f\\': 59.4, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Sunny\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/113.png\\', \\'code\\': 1000}, \\'wind_mph\\': 14.3, \\'wind_kph\\': 23.0, \\'wind_degree\\': 258, \\'wind_dir\\': \\'WSW\\', \\'pressure_mb\\': 1012.0, \\'pressure_in\\': 29.88, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 77, \\'cloud\\': 2, \\'feelslike_c\\': 14.5, \\'feelslike_f\\': 58.1, \\'windchill_c\\': 14.5, \\'windchill_f\\': 58.1, \\'heatindex_c\\': 15.2, \\'heatindex_f\\': 59.4, \\'dewpoint_c\\': 11.4, \\'dewpoint_f\\': 52.6, \\'vis_km\\': 10.0, \\'vis_miles\\': 6.0, \\'uv\\': 5.0, \\'gust_mph\\': 17.7, \\'gust_kph\\': 28.5}}, Log a trace. No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. Python. TypeScript. from langchain_openai import ChatOpenAI. from langchain_core.prompts import ChatPromptTemplate. from langchain_core.output_parsers import StrOutputParser. prompt = ChatPromptTemplate.from_messages([., To save your prompt, click the \"Save as\" button, name your prompt, and decide if you want it to be \"private\" or \"public\". Private prompts are only visible to your workspace, while public prompts are discoverable to anyone in the LangChain Hub. Click save to create your prompt. The model and configuration you select in the Playground settings ...'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a3258",
   "metadata": {},
   "source": [
    "Now let's examine the artifact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd25a2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'weather in sf?',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': 'The current weather in San Francisco is sunny with a temperature of 15.2°C (59.4°F). The wind speed is 23.0 kph coming from the WSW direction. The humidity is at 77%, and there is a visibility of 10.0 km (6.0 miles).',\n",
       " 'images': ['https://www.sftourismtips.com/image-files/sf-weather-graphic-july.jpg',\n",
       "  'https://www.weather2visit.com/images/charts/small/san-francisco-tmp-us.png',\n",
       "  'http://www.cpc.ncep.noaa.gov/products/analysis_monitoring/regional_monitoring/clreat.gif',\n",
       "  'https://cdn.hikb.at/charts/meteo-average-weather-weekly/sanfrancisco-meteo-average-weather-weekly.png?quality=5',\n",
       "  'https://lh5.googleusercontent.com/T_I7_3pbFU3dQzN66OfxPFO7cMkgL-KDPergpTWqPAsnzuNvDOeKzX4gYNWxcnxnrZaC7UqulGi-e9w4Mbg8merKoSmcNoAtDB6zZdMJIS_boAdrzB__hBy_pYa03cyMEJdon1p1'],\n",
       " 'results': [{'title': 'Weather in San Francisco',\n",
       "   'url': 'https://www.weatherapi.com/',\n",
       "   'content': \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.78, 'lon': -122.42, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1722043273, 'localtime': '2024-07-26 18:21'}, 'current': {'last_updated_epoch': 1722042900, 'last_updated': '2024-07-26 18:15', 'temp_c': 15.2, 'temp_f': 59.4, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 14.3, 'wind_kph': 23.0, 'wind_degree': 258, 'wind_dir': 'WSW', 'pressure_mb': 1012.0, 'pressure_in': 29.88, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 77, 'cloud': 2, 'feelslike_c': 14.5, 'feelslike_f': 58.1, 'windchill_c': 14.5, 'windchill_f': 58.1, 'heatindex_c': 15.2, 'heatindex_f': 59.4, 'dewpoint_c': 11.4, 'dewpoint_f': 52.6, 'vis_km': 10.0, 'vis_miles': 6.0, 'uv': 5.0, 'gust_mph': 17.7, 'gust_kph': 28.5}}\",\n",
       "   'score': 0.6738742,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Trace with LangChain (Python and JS/TS) | ️ ️ LangSmith',\n",
       "   'url': 'https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain',\n",
       "   'content': 'Log a trace. No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would. Python. TypeScript. from langchain_openai import ChatOpenAI. from langchain_core.prompts import ChatPromptTemplate. from langchain_core.output_parsers import StrOutputParser. prompt = ChatPromptTemplate.from_messages([.',\n",
       "   'score': 0.027795622,\n",
       "   'raw_content': 'Trace with LangChain (Python and JS/TS)\\nLangSmith integrates seamlessly with LangChain (Python and JS), the popular open-source framework for building LLM applications.\\nInstallation\\u200b\\nInstall the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below).\\nFor a full list of packages available, see the LangChain Python docs and LangChain JS docs.\\nQuick start\\u200b\\n1. Configure your environment\\u200b\\n2. Log a trace\\u200b\\nNo extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would.\\n3. View your trace\\u200b\\nBy default, the trace will be logged to the project with the name default. An example of a trace logged using the above code is made public and can be viewed here.\\nTrace selectively\\u200b\\nThe previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application.\\nThere are two ways to do this in Python: by manually passing in a LangChainTracer (reference docs) instance as a callback, or by using the tracing_v2_enabled context manager (reference docs).\\nIn JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback.\\nLog to a specific project\\u200b\\nStatically\\u200b\\nAs mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application.\\nDynamically\\u200b\\nThis largely builds off of the previous section and allows you to set the project name for a specific LangChainTracer instance or as parameters to the tracing_v2_enabled context manager in Python.\\nAdd metadata and tags to traces\\u200b\\nLangSmith supports sending arbitrary metadata and tags along with traces.\\nThis is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it.\\nFor information on how to query traces and runs by metadata and tags, see this guide\\nWhen you attach metadata or tags to a runnable (either through the RunnableConfig or at runtime with invocation params), they are inherited by all child runnables of that runnable.\\nCustomize run name\\u200b\\nWhen you create a run, you can specify a name for the run. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI.\\nThis can be done by setting a run_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS.\\nThis feature is not currently supported directly for LLM objects.\\nAccess run (span) ID for LangChain invocations\\u200b\\nWhen you invoke a LangChain object, you can access the run ID of the invocation. This run ID can be used to query the run in LangSmith.\\nIn Python, you can use the collect_runs context manager to access the run ID.\\nIn JS/TS, you can use a RunCollectorCallbackHandler instance to access the run ID.\\nEnsure all traces are submitted before exiting\\u200b\\nIn LangChain Python, LangSmith\\'s tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes.\\nIn LangChain JS/TS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to \"true\".\\nFor both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application.\\nBelow is an example:\\nTrace without setting environment variables\\u200b\\nAs mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project:\\nHowever, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically.\\nThis largely builds off of the previous section.\\nInteroperability between LangChain.JS and LangSmith SDK\\u200b\\nTracing LangChain objects inside traceable (JS only)\\u200b\\nStarting with langchain@0.2.x, LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function.\\nFor older versions of LangChain below 0.2.x, you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable.\\nManually tracing LangChain child runs via traceable / RunTree API (JS only)\\u200b\\nIn some uses cases, you might want to run traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API.\\nYou can convert the existing LangChain RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or you can pass the RunnableConfig as the first argument of the wrapped function, which will result in the following trace tree:'},\n",
       "  {'title': 'Self-querying retrievers | ️ LangChain',\n",
       "   'url': 'https://python.langchain.com/v0.1/docs/integrations/retrievers/self_query/',\n",
       "   'content': 'DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and ...',\n",
       "   'score': 0.016593179,\n",
       "   'raw_content': 'Self-querying retrievers\\nLearn about how the self-querying retriever works here.\\n📄️ Deep Lake\\nDeep Lake is a multimodal database for building AI applications\\n📄️ Astra DB (Cassandra)\\nDataStax Astra DB is a serverless vector-capable database built on Cassandra and made conveniently available through an easy-to-use JSON API.\\n📄️ Chroma\\nChroma is a vector database for building AI applications with embeddings.\\n📄️ DashVector\\nDashVector is a fully managed vector DB service that supports high-dimension dense and sparse vectors, real-time insertion and filtered search. It is built to scale automatically and can adapt to different application requirements.\\n📄️ Databricks Vector Search\\nDatabricks Vector Search is a serverless similarity search engine that allows you to store a vector representation of your data, including metadata, in a vector database. With Vector Search, you can create auto-updating vector search indexes from Delta tables managed by Unity Catalog and query them with a simple API to return the most similar vectors.\\n📄️ DingoDB\\nDingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.\\n📄️ Elasticsearch\\nElasticsearch is a distributed, RESTful search and analytics engine.\\n📄️ Milvus\\nMilvus is a database that stores, indexes, and manages massive embedding vectors generated by deep neural networks and other machine learning (ML) models.\\n📄️ MongoDB Atlas\\nMongoDB Atlas is a document database that can be\\n📄️ MyScale\\nMyScale is an integrated vector database. You can access your database in SQL and also from here, LangChain.\\n📄️ OpenSearch\\nOpenSearch is a scalable, flexible, and extensible open-source software suite for search, analytics, and observability applications licensed under Apache 2.0. OpenSearch is a distributed search and analytics engine based on Apache Lucene.\\n📄️ PGVector (Postgres)\\nPGVector is a vector similarity search package for Postgres data base.\\n📄️ Pinecone\\nPinecone is a vector database with broad functionality.\\n📄️ Qdrant\\nQdrant (read: quadrant) is a vector similarity search engine. It provides a production-ready service with a convenient API to store, search, and manage points - vectors with an additional payload. Qdrant is tailored to extended filtering support.\\n📄️ Redis\\nRedis is an open-source key-value store that can be used as a cache, message broker, database, vector database and more.\\n📄️ Supabase (Postgres)\\nSupabase is an open-source Firebase alternative.\\n📄️ Tencent Cloud VectorDB\\nTencent Cloud VectorDB is a fully managed, self-developed, enterprise-level distributed database\\nservice designed for storing, retrieving, and analyzing multi-dimensional vector data.\\n📄️ Timescale Vector (Postgres)\\nTimescale Vector is PostgreSQL++ for AI applications. It enables you to efficiently store and query billions of vector embeddings in PostgreSQL.\\n📄️ Vectara\\nVectara is the trusted GenAI platform that provides an easy-to-use API for document indexing and querying.\\n📄️ Weaviate\\nWeaviate is an open-source vector database. It allows you to store data objects and vector embeddings from'}],\n",
       " 'response_time': 4.06}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac8146c",
   "metadata": {},
   "source": [
    "Inside our artifact we have access to the `results` key which provides us more specific information on each of the search results returned by our Tavily tool.\n",
    "\n",
    "In addition we can see, the `answer`, `raw_content`, and `images` fields are populated because we set those parameters to true when we instantiated the tool. You can turn this off by setting the corresponding `include_answer`, `include_raw_content`, and `include_images` falgs to False. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
